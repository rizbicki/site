[{"authors":["Rafael Izbicki, PhD","Gilson Shimizu","Rafael B. Stern"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"02315d8270b0ce844e97d8328e21f88e","permalink":"http://www.rizbicki.ufscar.br/publication/2022_cdsplit/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022_cdsplit/","section":"publication","summary":"Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. We introduce CD-split and HPD-split, which yield general prediction regions and converge to the optimal highest predictive density set.","tags":["Conformal prediction","Machine Learning"],"title":"CD-split and HPD-split: Efficient conformal regions in high dimensions","type":"publication"},{"authors":["N. Dalmasso","L. Masserano","D. Zhao","Rafael Izbicki, PhD","A. B. Lee"],"categories":null,"content":"","date":1649462580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649462580,"objectID":"9bec034c426077100534798887e48a39","permalink":"http://www.rizbicki.ufscar.br/publication/bff/","publishdate":"2022-04-09T00:03:00Z","relpermalink":"/publication/bff/","section":"publication","summary":"Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, such as the likelihood ratio, can leverage the LF2I machinery to create valid confidence sets and diagnostics without costly Monte Carlo samples at fixed parameter settings. We study the power of two test statistics (ACORE and BFF), which, respectively, maximize versus integrate an odds function over the parameter space. Our paper discusses the benefits and challenges of LF2I, with a breakdown of the sources of errors in LF2I confidence sets.","tags":["Nonparametric Inference","Hypothesis Tests","Machine Learning","Diagnostics","ABC"],"title":"Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage","type":"publication"},{"authors":null,"categories":["Blog Post"],"content":"“Quase todo mundo que está hospitalizado é vacinado. Vacinas não funcionam =(” Esse pensamento passa na cabeça de muita gente e é muito usado por anti-vacinas. Mas o argumento é correto? NÃO! Essa falácia é chamada de base rate fallacy. Mostro aqui o porquê.\n{style=“display: block; margin: 0 auto”}\nVamos começar com um exemplo simples: como todas as vacinas têm uma chance de falhar, se todo mundo fosse vacinado, os poucos casos de internações necessariamente ocorreriam em vacinados. Isso daria a impressão que a vacina não funciona. O mesmo vale em um cenário mais realista. Por exemplo, essa figura mostra quantos internados vacinados esperamos observar em uma população com cobertura 95% de uma vacina com eficácia 90%. A maioria dos internados é vacinado.\n{style=“display: block; margin: 0 auto”}\nA conta para avaliar essa proporção vem do Teorema de Bayes e é dada pela seguinte equação, em que eᵥ denota a eficácia da vacina e P(Vacinado) é a proporção de vacinados.\n“Ué, mas cadê a vacina funcionando?” Aí vem o interessante: o número total internados é que diminuiu com a vacinação. Essencialmente, a eficácia da vacina pode ser pensada em termos de quantas hospitalizações foram evitadas. Veja como a comparação fica para os números acima.\n{style=“display: block; margin: 0 auto”} Fiz aqui um app para quem quiser brincar com os valores de eficácia/cobertura vacinal para ver o que acontece em termos de hospitalizações observadas e evitadas: https://t.co/nEDshfptsW.\n“Mas se não observamos o que teria acontecido sem as vacinas, como conseguimos estimar sua eficácia?” Para isso precisamos de experimentos e/ou modelos mais complexos. Por exemplo, na fase 3 do desenvolvimento de uma vacina, sorteamos quem recebe vacina ou placebo.\nCom isso conseguimos avaliar quantas internações ocorreram a mais em um grupo que no outro. Avaliar as vacinas em funcionamento com dados reais observados em hospitais é mais complexo e exige modelos estatísticos mais elaborados e com mais suposições.\nPor exemplo, em https://pubmed.ncbi.nlm.nih.gov/34495082/, utilizamos um grupo etário não vacinado para avaliar o efeito da vacina em idosos em São Paulo:\nPara estimar a eficácia a partir de dados observados (e não controlados por um experimento) precisamos saber mais do que a proporção de vacinados na população. Esse é um assunto complexo e fica para outro post. {style=“display: block; margin: 0 auto”}\nResumindo: Haver muitos vacinados entre internados não mostra que a vacina não funciona. Isso é uma falácia. Para avaliar adequadamente a eficácia de uma vacina, precisamos de experimentos mais complexos (como os de fase 3) e/ou modelos estatísticos mais elaborados ","date":1643155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643155200,"objectID":"3863753b2cd96b05354e2e0d8ca49076","permalink":"http://www.rizbicki.ufscar.br/post/base_rate_fallacy/","publishdate":"2022-01-26T00:00:00Z","relpermalink":"/post/base_rate_fallacy/","section":"post","summary":"“Quase todo mundo que está hospitalizado é vacinado. Vacinas não funcionam =(” Esse pensamento passa na cabeça de muita gente e é muito usado por anti-vacinas. Mas o argumento é correto?","tags":["Portuguese"],"title":"Base rate fallacy","type":"post"},{"authors":["D. Zhao","N. Dalmasso","Rafael Izbicki, PhD","A. B. Lee"],"categories":["paper"],"content":"","date":1631145780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631145780,"objectID":"3a81fb549a47a0becf072f71241bf3c0","permalink":"http://www.rizbicki.ufscar.br/publication/alp/","publishdate":"2021-09-09T00:03:00Z","relpermalink":"/publication/alp/","section":"publication","summary":"","tags":["Conditional Density","Nonparametric Inference","Approximate Likelihood","ABC","Density Estimation","Diagnostics"],"title":"Diagnostics for Conditional Density Models and Bayesian Inference Algorithms","type":"publication"},{"authors":["Rafael Izbicki, PhD","Joseph B. Kadane"],"categories":null,"content":" ","date":1622552400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622552400,"objectID":"81f50af9d037aff3952900ed6e9f9184","permalink":"http://www.rizbicki.ufscar.br/talk/indemnity-for-a-lost-chance/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/talk/indemnity-for-a-lost-chance/","section":"event","summary":"Civil liability for a lost chance applies to cases in which a tortious action changes the probabilities of the outcomes that can be obtained by the victim. This talk uses Decision Theory to discuss how to quantify damages in such cases.","tags":["Decision Theory","Jurimetrics","Lost Chance","Causality"],"title":"Indemnity for a lost chance","type":"event"},{"authors":["Mateus B. Comito","Rafael Izbicki, PhD","Rafael B. Stern","Julio A. Z. Trecenti"],"categories":null,"content":"","date":1613692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613692800,"objectID":"e2f5297a833194797730968ed11429e3","permalink":"http://www.rizbicki.ufscar.br/publication/2021_desaparecimento/","publishdate":"2021-02-19T00:00:00Z","relpermalink":"/publication/2021_desaparecimento/","section":"publication","summary":"Este artigo estuda as causas de desaparecimento no estado de São Paulo por idade e sexo dos desaparecidos. Para tanto, utiliza algoritmos de aprendizado de máquina para avaliar automaticamente o texto de boletins de ocorrência.","tags":["Jurimetrics","Missing people","Machine Learning","Quantification"],"title":"Causas de desaparecimento no estado de São Paulo entre 2013 e 2014: uma análise automatizada de boletins de ocorrência","type":"publication"},{"authors":["T. McNeely","G. Vincent","Rafael Izbicki, PhD","K. M. Wood","A. B. Lee"],"categories":null,"content":"","date":1612828980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612828980,"objectID":"f5a362172dae36cbabe7759cbf5cf27c","permalink":"http://www.rizbicki.ufscar.br/publication/cyclones/","publishdate":"2021-02-09T00:03:00Z","relpermalink":"/publication/cyclones/","section":"publication","summary":"Tropical cyclone (TC) intensity forecasts are issued by human forecasters who evaluate spatio-temporal observations (e.g., satellite imagery) and model output (e.g., numerical weather prediction, statistical models) to produce forecasts every 6 hours. Within these time constraints, it can be challenging to draw insight from such data. While high-capacity machine learning methods are well suited for prediction problems with complex sequence data, extracting interpretable scientific information with such methods is difficult. Here we leverage powerful AI prediction algorithms and classical statistical inference to identify patterns in the evolution of TC convective structure leading up to the rapid intensification of a storm, hence providing forecasters and scientists with key insight into TC behavior.","tags":["Nonparametric Inference","Hypothesis Tests","Machine Learning","Diagnostics"],"title":"Identifying Distributional Differences in Convective Evolution Prior to Rapid Intensification in Tropical Cyclones","type":"publication"},{"authors":["D. Valle","G. Shimizu","Rafael Izbicki, PhD","L. Maracahipes","D. Silvério","L. Paolucci","Y. Jameel","P. Brando"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"0378c3bc991d5f8d5f7f15604c130a35","permalink":"http://www.rizbicki.ufscar.br/publication/lda/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/lda/","section":"publication","summary":"Understanding and predicting the effect of global change phenomena on biodiversity is challenging given that biodiversity data are highly multivariate, containing information from tens to hundreds of species in any given location and time. The Latent Dirichlet Allocation (LDA) model has been recently proposed to decompose biodiversity data into latent communities. While LDA is a very useful exploratory tool and overcomes several limitations of earlier methods, it has limited inferential and predictive skill given that covariates cannot be included in the model. We introduce a modified LDA model (called LDAcov) which allows the incorporation of covariates, enabling inference on the drivers of change of latent communities, spatial interpolation of results, and prediction based on future environmental change scenarios. We show with simulated data that our approach to fitting LDAcov is able to estimate well the number of groups and all model parameters. We illustrate LDAcov using data from two experimental studies on the long-term effects of fire on southeastern Amazonian forests in Brazil. Our results reveal that repeated fires can have a strong impact on plant assemblages, particularly if fuel is allowed to build up between consecutive fires. The effect of fire is exacerbated as distance to the edge of the forest decreases, with small-sized species and species with thin bark being impacted the most. These results highlight the compounding impacts of multiple fire events and fragmentation, a scenario commonly found across the southern edge of Amazon. We believe that LDAcov will be of wide interest to scientists studying the effect of global change phenomena on biodiversity using high-dimensional datasets. Thus, we developed the R package LDAcov to enable the straightforward use of this model.","tags":["LDA","Machine Learning"],"title":"The Latent Dirichlet Allocation model with covariates (LDAcov): a case study on the effect of fire on species composition in Amazonian forests","type":"publication"},{"authors":["M. H. de A. Inacio","Rafael Izbicki, PhD","B. Gyires-Tóth"],"categories":null,"content":"","date":1609460400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609460400,"objectID":"04bfbace8674d00e2ceccb5d84186325","permalink":"http://www.rizbicki.ufscar.br/publication/vae_test/","publishdate":"2021-01-01T00:20:00Z","relpermalink":"/publication/vae_test/","section":"publication","summary":"An important question in many machine learning applications is whether two samples arise from the same generating distribution. Although an old topic in Statistics, simple accept/reject decisions given by most hypothesis tests are often not enough: it is well known that the rejection of the null hypothesis does not imply that differences between the two groups are meaningful from a practical perspective. In this work, we present a novel nonparametric approach to visually assess the dissimilarity between the datasets that goes beyond two-sample testing. The key idea of our approach is to measure the distance between two (possibly) high-dimensional datasets using variational autoencoders. We also show how this framework can be used to create a formal statistical test to test the hypothesis that both samples arise from the same distribution. We evaluate both the distance measurement and hypothesis testing approaches on simulated and real world datasets. The results show that our approach is useful for data exploration (as it, for instance, allows for quantification of the discrepancy/separability between categories of images), which can be particularly helpful in early phases of the a machine learning pipeline.","tags":["Machine Learning","Neural Networks","Hypothesis Testing"],"title":"Distance assessment and analysis of high-dimensional samples using variational autoencoders","type":"publication"},{"authors":["Rafael Izbicki, PhD","L. S. Bastos","M. Izbicki","H. F. Lopes","T. M. dos Santos"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"d61b49d9b3b8907afc58ef354f88ae39","permalink":"http://www.rizbicki.ufscar.br/publication/vaccine_sao_paulo/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/vaccine_sao_paulo/","section":"publication","summary":"","tags":[],"title":"How many hospitalizations has the COVID-19 vaccination already prevented in São Paulo?","type":"publication"},{"authors":["M. H. de A. Inacio","Rafael Izbicki, PhD","D. L. Lopes","M. A. Diniz","L. E. B. Salasar","J. Poloniato"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"56f9026eefab8275961f20f35d8a34fb","permalink":"http://www.rizbicki.ufscar.br/publication/fifa/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/fifa/","section":"publication","summary":"","tags":["Bayesian Statistics","Sports","Wisdom of Crowds","Crowdsourcing"],"title":"What if the forecaster knew? Assessing forecast reliability via simulation","type":"publication"},{"authors":["Rafael Izbicki, PhD","M. A. Diniz","L. S Bastos"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"dab0e03fac5a4de6c2cde996333a5daa","permalink":"http://www.rizbicki.ufscar.br/publication/covid_prevalence/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/covid_prevalence/","section":"publication","summary":"","tags":[],"title":"Sensitivity and specificity in prevalence studies: the importance of considering uncertainty","type":"publication"},{"authors":null,"categories":null,"content":" Aqui você pode baixar o livro Aprendizado de máquina: uma abordagem estatística, escrito por mim e pelo Tiago Mendonça (ISBN 978-65-00-02410-4). A capa do livro foi feita pelos incríveis Leonardo M. Borges e Kaori Nagata.\nCaso queira imprimir o livro, utilize a versão deste link, que possui sinalização de onde ele deve ser cortado. Sugerimos utilizar uma resolução de ao menos 500dpi.\nCitação: Izbicki, R. e Santos, T. M. dos. Aprendizado de máquina: uma abordagem estatística. 1ᵃ edição. 2020. 272 páginas. ISBN: 978-65-00-02410-4. Para a entrada em bibtex, clique aqui.\nVídeos (em desenvolvimento) Playlist:\nVídeos de cada capítulo:\nCapítulos 1 e 2 Introdução Elementos de um problema de predição e regressão linear Overfitting Seleção de modelos Mais sobre data-splitting Mais sobre o risco Capítulo 3 Lasso e seleção de variáveis Mais sobre penalização Capítulo 4 KNN, Nadaraya-Watson, Séries Ortogonais Árvores, Bagging e Florestas Aleatórias Redes Neurais e Deep Learning RKHS, SVR e KRR Truque do Kernel Teoria Capítulo 5 Maldição da dimensionalidade Capítulo 6 Interpretabilidade/ExplainableML Inferência conformal Estimação de densidades condicionais Capítulos 7 e 8 Risco, regressão logística e classificação via regressão Seleção de modelos, outras medidas de desempenho e desbalanceamento Outros classificadores plugin: Bayes ingênuo e análise discriminante SVM KNN, árvores, florestas e redes neurais Boosting Capítulo 9 Assimetria na função de perda e dados desbalanceados Dataset shift e viés de seleção Combinando classificadores Teoria VC Capítulo 10 PCA, KPCA e projeções aleatórias Autoencoders Capítulo 11 K-medias, clustering espectral e agrupamento hierárquico Capítulo 12 Regras de associação Capítulo 13 Sistemas de recomendação Apêndice Manipulando textos e imagens Bancos de dados Estes são os bancos de dados usados no livro:\nAmazon Fine Food reviews Hour Indicadores SSP Prostate World Development Indicators Spam ","date":1595894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595894400,"objectID":"3b31f61458a3eeb5c66a1cba43fc4c01","permalink":"http://www.rizbicki.ufscar.br/ame/","publishdate":"2020-07-28T00:00:00Z","relpermalink":"/ame/","section":"","summary":"Aqui você pode baixar o livro Aprendizado de máquina: uma abordagem estatística, escrito por mim e pelo Tiago Mendonça (ISBN 978-65-00-02410-4). A capa do livro foi feita pelos incríveis Leonardo M.","tags":null,"title":"","type":"page"},{"authors":null,"categories":["Blog Post"],"content":" Fiz um dashboard que mostra a evolução do coronavírus por município e estado brasileiro, pois senti falta de alguns gráficos para descrever essa dinâmica de forma mais localizada:\nEle é útil não só para ver cada município separadamente, mas também para entender como o vírus está se espalhando no Brasil. Ele também mostra a evolução por semana, que acho ser mais robusta por remover o efeito que os finais de semana têm no sistema de notificação. Além disso, também inclui a evolução normalizada por data de início (medida pelo dia do 15º óbito). Adicionei também informação sobre a variação no número de casos/óbitos por município.\nEle está disponível em https://rizbicki.shinyapps.io/covid_por_localidade/\n","date":1588377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588377600,"objectID":"4b94e4c21f873e73a4cbca5781e9b7f8","permalink":"http://www.rizbicki.ufscar.br/post/app_covid_localidade/","publishdate":"2020-05-02T00:00:00Z","relpermalink":"/post/app_covid_localidade/","section":"post","summary":"Fiz um dashboard que mostra a evolução do coronavírus por município e estado brasileiro, pois senti falta de alguns gráficos para descrever essa dinâmica de forma mais localizada:\nEle é útil não só para ver cada município separadamente, mas também para entender como o vírus está se espalhando no Brasil.","tags":["Portuguese","covid-19","Dashboard"],"title":"Evolução da Covid-19 no Brasil","type":"post"},{"authors":["Victor Coscrato","Rafael Izbicki, PhD","Rafael B. Stern"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"0bc55976595a53c6a2b827c2d8e58a3f","permalink":"http://www.rizbicki.ufscar.br/publication/2020_agnostic_error/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020_agnostic_error/","section":"publication","summary":"Despite its common practice, statistical hypothesis testing presents challenges in interpretation. For instance, in the standard frequentist framework there is no control of the type II error. As a result, the non-rejection of the null hypothesis (H0) cannot reasonably be interpreted as its acceptance. We propose that this dilemma can be overcome by using agnostic hypothesis tests, since they can control the type I and II errors simultaneously. In order to make this idea operational, we show how to obtain agnostic hypothesis in typical models. For instance, we show how to build (unbiased) uniformly most powerful agnostic tests and how to obtain agnostic tests from standard p-values. Also, we present conditions such that the above tests can be made logically coherent. Finally, we present examples of consistent agnostic hypothesis tests.","tags":["Foundations","Hypothesis Test","Agnostic Test","UMP Test","Region Test","Logical Coherence"],"title":"Agnostic tests can control the type I and type II errors simultaneously","type":"publication"},{"authors":["Gilson Shimizu","Rafael Izbicki, PhD","Rafael B. Stern"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"93be656354df7bf2ca55aeb8820fb8b0","permalink":"http://www.rizbicki.ufscar.br/publication/2020_cdsplit_aistats/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020_cdsplit_aistats/","section":"publication","summary":"Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Besides average coverage, one might also desire to control conditional coverage, that is, coverage for every new testing point. However, without strong assumptions, conditional coverage is unachievable. Given this limitation, the literature has focused on methods with asymptotical conditional coverage. In order to obtain this property, these methods require strong conditions on the dependence between the target variable and the features. We introduce two conformal methods based on conditional density estimators that do not depend on this type of assumption to obtain asymptotic conditional coverage: Dist-split and CD-split. While Dist-split asymptotically obtains optimal intervals, which are easier to interpret than general regions, CD-split obtains optimal size regions, which are smaller than intervals. CD-split also obtains local coverage by creating prediction bands locally on a partition of the features space. This partition is data-driven and scales to high-dimensional settings. In a wide variety of simulated scenarios, our methods have a better control of conditional coverage and have smaller length than previously proposed methods.","tags":["Conformal Prediction","Machine Learning","CD-Split"],"title":"Flexible distribution-free conditional predictive bands using density estimators","type":"publication"},{"authors":null,"categories":["R","machine learning","Blog Post"],"content":" In this post I analyse the covid-19 data from https://www.kaggle.com/einsteindata4u/covid19, which contains information about patients from Albert Einstein’s Hospital, in São Paulo (Brazil).\nMy main assumptions in the following analysis are that:\nThe goal is to provide a screening protocol that will reduce the number of patients in which the covid-19 PCR is applied to. Thus, a large sensitivity (probability of correctly identifying a true positive) is more important than a large specificity (probability of correctly identifying a true negative).\nI’m assuming that blood count exams and exams for detecting the presence of other virus were applied at random to each patient, i.e., they are not applied according to relevant factors related to covid-19. This is a STRONG assumption.\nI’m assuming that all data provided is about suspect cases.\nI’m assuming that blood count exams are fast and cheap, while exams for detecting the presence of other virus/bacteria are more expensive.\nIn short, the approach I’ll take consists in\nProvide a screening technique using only the blood count exam and age. This will already remove about half of the patients from the list of suspects IF THE ASSUMPTIONS ARE CORRECT and the prevalence of covid-19 patients that go to the hospital does not change. As this prevalence is expected to increase, this protocol will probably be able to screen-out less patients every day.\nApply exams for detecting the presence of other virus/bacteria for patients that are not removed in step 1. Step 2 will only be useful if the cost/time to apply these exams is smaller than that to apply covid-19 PCR.\nData preprocessing library(tidyverse) library(readxl) library(naniar) library(randomForest) library(glmnet) library(ggpubr) library(plotROC) library(pROC) set.seed(0) data \u0026lt;- read_xlsx(\u0026#34;dataset.xlsx\u0026#34;) As there are many variables with missing data, I’ll start by removing those that have only been applied to less than 250 patients.\nkeep_cols \u0026lt;- apply(data,2,function(x)sum(!is.na(x)))\u0026gt;=250 data \u0026lt;- data[,keep_cols] vis_miss(data) Given the missing patter (which consists of blocks of covariates), for simplicity we will keep only (i) age, (ii) the results from the complete blood count and (iii) the results for detecting other virus/bacteria.\nScreening based on blood count First, let’s extract the data from the blood count exam and the age of the patient.\ndata_blood_count \u0026lt;- data %\u0026gt;% select(`Patient ID`,`SARS-Cov-2 exam result`,`Patient age quantile`,Hematocrit:`Red blood cell distribution width (RDW)`) data_blood_count \u0026lt;- data_blood_count[complete.cases(data_blood_count),] We will now split the sample into train/test.\nsplit \u0026lt;- sample(c(\u0026#34;Train\u0026#34;,\u0026#34;Test\u0026#34;),nrow(data_blood_count),prob = c(0.7,0.3),replace = TRUE) covariates_train \u0026lt;- data_blood_count %\u0026gt;% filter(split==\u0026#34;Train\u0026#34;) %\u0026gt;% select(-c(`Patient ID`,`SARS-Cov-2 exam result`)) covariates_test \u0026lt;- data_blood_count %\u0026gt;% filter(split==\u0026#34;Test\u0026#34;) %\u0026gt;% select(-c(`Patient ID`,`SARS-Cov-2 exam result`)) response_train \u0026lt;- data_blood_count%\u0026gt;% filter(split==\u0026#34;Train\u0026#34;) %\u0026gt;% select(`SARS-Cov-2 exam result`) %\u0026gt;% pull()==\u0026#34;positive\u0026#34; We will apply two probabilistic classifiers: (i) a random forest, which fully nonparametric and therefore is able to handle complex interactions between the covariates and (ii) a logistic regression with penalization, which is parametric and easier to be applies by practitioners. The penalization is important because of the small sample size. We will then compute ROC curves so that we can choose cutoffs that lead to reasonable sensitivity/specificity values for this application. Notice that accuracy (1-proportion of mistakes) is not a good measure for our goal.\nRandom forest First we fit the random forest and show the importance of each feature. Leukocytes, platets and eosinophils seem to be the most important features among those obtained in the blood count exam. Notice that we are treating this as a regression problem so that we can get a score between 0 and 1; the high imbalance of the classes would lead a naive classification forest to give results that are not useful for screening (because they assume a 0-1 loss function).\nfit_rf \u0026lt;- randomForest(x=covariates_train, y=response_train) varImpPlot(fit_rf) pred_rf \u0026lt;- predict(fit_rf,newdata = covariates_test) Logistic regression with L1 penalization Next, let’s apply the logistic regression. If we use the magnitude of the fitted coefficients are a measure of their importance (which is reasonable because all features are scaled), we get again that the most important features are leukocytes, platets and eosinophils.\nfit_logistic_l1 \u0026lt;- cv.glmnet(x=covariates_train %\u0026gt;%as.matrix(), y=response_train, family=\u0026#34;binomial\u0026#34;) pred_logistic_l1 \u0026lt;- predict(fit_logistic_l1,newx = covariates_test%\u0026gt;% as.matrix(), s=fit_logistic_l1$lambda.min, type=\u0026#34;response\u0026#34;) coefficients(fit_logistic_l1,s=fit_logistic_l1$lambda.min) ## 16 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) -3.814016733 ## Patient age quantile 0.092543471 ## …","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"a7b9de64f742165961fc5314ea09cee8","permalink":"http://www.rizbicki.ufscar.br/post/einstein/analysis/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/post/einstein/analysis/","section":"post","summary":"In this post I analyse the covid-19 data from https://www.kaggle.com/einsteindata4u/covid19, which contains information about patients from Albert Einstein’s Hospital, in São Paulo (Brazil).\nMy main assumptions in the following analysis are that:","tags":["R","machine learning","covid-19"],"title":"Covid-19 Einstein Analysis","type":"post"},{"authors":["Niccolò Dalmasso","Rafael Izbicki, PhD","Ann B. Lee"],"categories":null,"content":"","date":1580860980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580860980,"objectID":"744fcfce179e40caf207ad56e5ec52d2","permalink":"http://www.rizbicki.ufscar.br/publication/2020_confidence_likelihood_free/","publishdate":"2020-02-05T00:03:00Z","relpermalink":"/publication/2020_confidence_likelihood_free/","section":"publication","summary":"Parameter estimation, statistical tests and confidence sets are the cornerstones of classical statistics that allow scientists to make inferences about the underlying process that generated the observed data. A key question is whether one can still construct hypothesis tests and confidence sets with proper coverage and high power in a so-called likelihood-free inference (LFI) setting; that is, a setting where the likelihood is not explicitly known but one can forward-simulate observable data according to a stochastic model. We present ACORE, a frequentist approach to LFI that first formulates the classical likelihood ratio test (LRT) as a parametrized classification problem, and then uses the equivalence of tests and confidence sets to build confidence regions for parameters of interest. We also present a goodness-of-fit procedure for checking whether the constructed tests and confidence regions are valid.","tags":["Approximate Likelihood","Confidence Intervals","Machine Learning"],"title":"Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting","type":"publication"},{"authors":["N. Dalmasso","A. B. Lee","Rafael Izbicki, PhD","T. Pospisil","I. Kim","C. Lin"],"categories":null,"content":"","date":1578182580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578182580,"objectID":"f23aec8133e4efd54928899345cb8835","permalink":"http://www.rizbicki.ufscar.br/publication/validation/","publishdate":"2020-01-05T00:03:00Z","relpermalink":"/publication/validation/","section":"publication","summary":"Complex phenomena in engineering and the sciences are often modeled with computationally intensive feed-forward simulations for which a tractable analytic likelihood does not exist. In these cases, it is sometimes necessary to estimate an approximate likelihood or fit a fast emulator model for efficient statistical inference; such surrogate models include Gaussian synthetic likelihoods and more recently neural density estimators such as autoregressive models and normalizing flows. To date, however, there is no consistent way of quantifying the quality of such a fit. Here we propose a statistical framework that can distinguish any arbitrary misspecified model from the target likelihood, and that in addition can identify with statistical confidence the regions of parameter as well as feature space where the fit is inadequate. Our validation method applies to settings where simulations are extremely costly and generated in batches or 'ensembles' at fixed locations in parameter space. At the heart of our approach is a two-sample test that quantifies the quality of the fit at fixed parameter values, and a global test that assesses goodness-of-fit across simulation parameters. While our general framework can incorporate any test statistic or distance metric, we specifically argue for a new two-sample test that can leverage any regression method to attain high power and provide diagnostics in complex data settings.","tags":["Emulators","Validation","Approximate Likelihood","ABC"],"title":"Validation of Approximate Likelihood and Emulator Models for Computationally Intensive Simulations","type":"publication"},{"authors":["N. Dalmasso","T. Pospisil","A. B. Lee","Rafael Izbicki, PhD","P. E. Freeman","A. I. Malz"],"categories":null,"content":"","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"1aa854d23a981308a6e5adea7bec744a","permalink":"http://www.rizbicki.ufscar.br/publication/cde_tools/","publishdate":"2020-01-02T00:00:00Z","relpermalink":"/publication/cde_tools/","section":"publication","summary":"","tags":["Nonparametric Statistics","Density Ratio","Selection Bias","Astrostatistics","Photometric Redshift Prediction"],"title":"Conditional Density Estimation Tools in Python and R with Applications to Photometric Redshifts and Likelihood-Free Cosmological Inference","type":"publication"},{"authors":["V. A. Coscrato","M. H. de A. Inacio","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"8ca9eda502e82783cbd13c15ca2e1e4c","permalink":"http://www.rizbicki.ufscar.br/publication/meta_learning/","publishdate":"2020-01-02T00:00:00Z","relpermalink":"/publication/meta_learning/","section":"publication","summary":"Stacking methods improve the prediction performance of regression models. A simple way to stack base regressions estimators is by combining them linearly, as done by Breiman [1]. Even though this approach is useful from an interpretative perspective, it often does not lead to high predictive power. We propose the NN-Stacking method (NNS), which generalizes Breiman’s method by allowing the linear parameters to vary with input features. This improvement enables NNS to take advantage of the fact that distinct base models often perform better at different regions of the feature space. Our method uses neural networks to estimate the stacking coefficients. We show that while our approach keeps the interpretative features of Breiman’s method at a local level, it leads to better predictive power, especially in datasets with large sample sizes.","tags":["Machine Learning","Model Selection","Neural Networks"],"title":"The NN-Stacking: Feature weighted linear stacking through neural networks","type":"publication"},{"authors":["R. de C.  Ceregatti","Rafael Izbicki, PhD","L. E. B. Salasar"],"categories":null,"content":"","date":1577872800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577872800,"objectID":"2a6cdd3702221e7e9e6e11de823a8149","permalink":"http://www.rizbicki.ufscar.br/publication/wiks/","publishdate":"2020-01-01T10:00:00Z","relpermalink":"/publication/wiks/","section":"publication","summary":"A key problem in many research investigations is to decide whether two samples have the same distribution. Numerous statistical methods have been devoted to this issue, but only few considered a Bayesian nonparametric approach. In this paper, we propose a novel nonparametric Bayesian index (WIKS) for quantifying the difference between two populations 𝑃1 and 𝑃2, which is defined by a weighted posterior expectation of the Kolmogorov–Smirnov distance between 𝑃1 and 𝑃2. We present a Bayesian decision-theoretic argument to support the use of WIKS index and a simple algorithm to compute it. Furthermore, we prove that WIKS is a statistically consistent procedure and that it controls the significance level uniformly over the null hypothesis, a feature that simplifies the choice of cutoff values for taking decisions. We present a real data analysis and an extensive simulation study showing that WIKS is more powerful than competing approaches under several settings.","tags":["Nonparametric Statistics","Bayesian Inference","Hypothesis Testing"],"title":"WIKS: A general Bayesian nonparametric index for quantifying differences between two populations","type":"publication"},{"authors":["S. Schmidt","A. Malz","Et Al","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1577840400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577840400,"objectID":"cec2a4738325540ad7b6296bf9fa42ab","permalink":"http://www.rizbicki.ufscar.br/publication/desc/","publishdate":"2020-01-01T01:00:00Z","relpermalink":"/publication/desc/","section":"publication","summary":"","tags":["Nonparametric Statistics","Astrostatistics","Photometric Redshift Prediction","Conditional Density Estimation"],"title":"Evaluation of probabilistic photometric redshift estimation approaches for The Rubin Observatory Legacy Survey of Space and Time (LSST)","type":"publication"},{"authors":["L. M. Borges","V. C. Reis","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1577836803,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836803,"objectID":"be77f82d3c2abeac76b62e2f26a2bf97","permalink":"http://www.rizbicki.ufscar.br/publication/digital_morphology/","publishdate":"2020-01-01T00:00:03Z","relpermalink":"/publication/digital_morphology/","section":"publication","summary":"","tags":["Ecology"],"title":"Schrödinger's phenotypes: herbarium specimens show two-dimensional images are both good and (not so) bad sources of morphological data","type":"publication"},{"authors":["M. Musetti","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"77c547f625112f0b98b6e083d50d2984","permalink":"http://www.rizbicki.ufscar.br/publication/combinando/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/combinando/","section":"publication","summary":"","tags":["Machine Learning","Meta-learning"],"title":"Combinando métodos de aprendizado supervisionado para a melhoria da previsão do redshift de galáxia","type":"publication"},{"authors":["T. Botari","Rafael Izbicki, PhD","A. C. P. L. F. Carvalho"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"356b0b5f5309c1b653cb491dc692231d","permalink":"http://www.rizbicki.ufscar.br/publication/local_interpretation/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/local_interpretation/","section":"publication","summary":"As machine learning becomes an important part of many real world applications affecting human lives, new requirements, besides high predictive accuracy, become important. One important requirement is transparency, which has been associated with model interpretability. Many machine learning algorithms induce models difficult to interpret, named black box. Moreover, people have difficulty to trust models that cannot be explained. In particular for machine learning, many groups are investigating new methods able to explain black box models. These methods usually look inside the black models to explain their inner work. By doing so, they allow the interpretation of the decision making process used by black box models. Among the recently proposed model interpretation methods, there is a group, named local estimators, which are designed to explain how the label of particular instance is predicted. For such, they induce interpretable models on the neighborhood of the instance to be explained. Local estimators have been successfully used to explain specific predictions. Although they provide some degree of model interpretability, it is still not clear what is the best way to implement and apply them. Open questions include: how to best define the neighborhood of an instance? How to control the trade-off between the accuracy of the interpretation method and its interpretability? How to make the obtained solution robust to small variations on the instance to be explained? To answer to these questions, we propose and investigate two strategies: (i) using data instance properties to provide improved explanations, and (ii) making sure that the neighborhood of an instance is properly defined by taking the geometry of the domain of the feature space into account. We evaluate these strategies in a regression task and present experimental results that show that they can improve local explanations.","tags":["Machine Learning","Explainable ML","Interpretation"],"title":"Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space","type":"publication"},{"authors":["Luís G. Esteves","Rafael Izbicki, PhD","Julio M. Stern","Rafael B. Stern"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"d124d931a4a4ae23305176904d02314e","permalink":"http://www.rizbicki.ufscar.br/publication/2019_pragmatic/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/2019_pragmatic/","section":"publication","summary":"This paper introduces pragmatic hypotheses and relates this concept to the spiral of scientific evolution. Previous works determined a characterization of logically consistent statistical hypothesis tests and showed that the modal operators obtained from this test can be represented in the hexagon of oppositions. However, despite the importance of precise hypothesis in science, they cannot be accepted by logically consistent tests. Here, we show that this dilemma can be overcome by the use of pragmatic versions of precise hypotheses. These pragmatic versions allow a level of imprecision in the hypothesis that is small relative to other experimental conditions. The introduction of pragmatic hypotheses allows the evolution of scientific theories based on statistical hypothesis testing to be interpreted using the narratological structure of hexagonal spirals, as defined by Pierre Gallais.","tags":["Hypothesis Test","Pragmatic Hypothesis","Logical Coherence"],"title":"Pragmatic hypotheses in the evolution of Science","type":"publication"},{"authors":["Afonso F. Vaz","Rafael Izbicki, PhD","Rafael B. Stern"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"eb9f5f3c4051af8e0895284e6977cd2f","permalink":"http://www.rizbicki.ufscar.br/publication/2019_quantification/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/2019_quantification/","section":"publication","summary":"The quantification problem consists of determining the prevalence of a given label in a target population. However, one often has access to the labels in a sample from the training population but not in the target population. A common assumption in this situation is that of prior probability shift, that is, once the labels are known, the distribution of the features is the same in the training and target populations. In this paper, we derive a new lower bound for the risk of the quantification problem under the prior shift assumption. Using a weaker version of the prior shift assumption, which can be tested, we show that ratio estimators can be used to build confidence intervals for the quantification problem.","tags":["Quantification","Prior Shift","Machine Learning"],"title":"Quantification under prior probability shift: the ratio estimator and its extensions","type":"publication"},{"authors":["Rafael Izbicki, PhD","Taylor Pospisil","Ann B. Lee"],"categories":null,"content":"","date":1549324980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324980,"objectID":"f8f3bacab64ef259580a9231b596d5c5","permalink":"http://www.rizbicki.ufscar.br/publication/2019_abccde/","publishdate":"2019-02-05T00:03:00Z","relpermalink":"/publication/2019_abccde/","section":"publication","summary":"We show how a nonparametric conditional density estimation (CDE) framework helps address three nontrivial challenges in ABC. (i) how to efficiently estimate the posterior distribution with limited simulations and different types of data,  (ii) how to tune and compare the performance of ABC and related methods in estimating the posterior itself, rather than just certain properties of the density, and (iii) how to efficiently choose among a large set of summary statistics based on a CDE surrogate loss.","tags":["Nonparametric Statistics","Likelihood Estimation","ABC","Approximate Bayesian Computation"],"title":"ABC-CDE: Toward Approximate Bayesian Computation with Complex High-Dimensional Data and Limited Simulations","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://www.rizbicki.ufscar.br/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["H. G. dos Santos","C. F. do Nascimento","Rafael Izbicki, PhD","Y. A. de O. Duarte","A. D. P. Chiavegatto Filho"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"545d72a7aa913c5cdd9e4d7e02b18f12","permalink":"http://www.rizbicki.ufscar.br/publication/obitos/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/obitos/","section":"publication","summary":"","tags":[],"title":"Machine learning para análises preditivas em saúde: exemplo de aplicação para predizer óbito em idosos de São Paulo","type":"publication"},{"authors":["Couto","C. M. V.","Cumming","G. S.","Lacorte G.","Congrains","C.","Rafael Izbicki, PhD","Braga","E. M.","Rocha","C. D.","Moralez-Silva","E.","Henry","D. A. W.","Manu","S. A.","Abalaka","J.","Regalla","A.","Silva","A. S.","Diop","M.","Lama","S. N. del."],"categories":null,"content":"","date":1546300801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300801,"objectID":"39632ee5b70706fc3b914f51c722b6b6","permalink":"http://www.rizbicki.ufscar.br/publication/africa/","publishdate":"2019-01-01T00:00:01Z","relpermalink":"/publication/africa/","section":"publication","summary":"","tags":["Biology","Ecology"],"title":"Avian haemosporidians in the cattle egret (Bubulcus ibis) from central-western and southern Africa: high diversity and prevalence","type":"publication"},{"authors":["M. A. Diniz","Rafael Izbicki, PhD","D. L. Lopes","L. E. B. Salasar"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4f44277ec1663a5ba1dda8eae2eeaf42","permalink":"http://www.rizbicki.ufscar.br/publication/football/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/football/","section":"publication","summary":"We propose two Bayesian multinomial-Dirichlet models to predict the final outcome of football (soccer) matches and compare them to three well-known models regarding their predictive power. All the models predicted the full-time results of 1710 matches of the first division of the Brazilian football championship and the comparison used three proper scoring rules, the proportion of errors and a calibration assessment. We also provide a goodness of fit measure. Our results show that multinomial-Dirichlet models are not only competitive with standard approaches, but they are also well calibrated and present reasonable goodness of fit.","tags":["Bayesian Statistics","Sports"],"title":"Comparing probabilistic predictive models applied to football","type":"publication"},{"authors":null,"categories":null,"content":"Material usado no minicurso Statistical Machine Learning.\nMachine Learning sob a ótica estatística: Uma abordagem preditivista para estatística com exemplos em R, com Tiago Mendonça dos Santos.\nSlides\nAnálises Amazon\nShiny Modelo Correto/Lasso\n","date":1544140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544140800,"objectID":"2d6019bb455e8e68926b4f816133716b","permalink":"http://www.rizbicki.ufscar.br/minicurso_sml/","publishdate":"2018-12-07T00:00:00Z","relpermalink":"/minicurso_sml/","section":"","summary":"Material usado no minicurso Statistical Machine Learning.\nMachine Learning sob a ótica estatística: Uma abordagem preditivista para estatística com exemplos em R, com Tiago Mendonça dos Santos.\nSlides\nAnálises Amazon","tags":null,"title":"","type":"page"},{"authors":["M. H. de A. Inácio","Rafael Izbicki, PhD","L. E. B. Salasar"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"ff56ecd37555610b030e273a0d753311","permalink":"http://www.rizbicki.ufscar.br/publication/density_fourier/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/density_fourier/","section":"publication","summary":"","tags":["Nonparametric Statistics","Density Estimation","Bayesian Inference"],"title":"Comparing two populations using Bayesian Fourier series density estimation","type":"publication"},{"authors":["M. H. de A. Inacio","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1527465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527465600,"objectID":"60c2e904aee8b563c0f9fb08bd5840d3","permalink":"http://www.rizbicki.ufscar.br/publication/flexcode_nnets/","publishdate":"2018-05-28T00:00:00Z","relpermalink":"/publication/flexcode_nnets/","section":"publication","summary":"Most machine learning tools aim at creating good predictions for new samples. However, obtaining 100% is not feasible in most problems, and therefore modeling the uncertainty over such predictions becomes necessary in several applications. This can be achieved by estimating conditional densities. In this work, we propose a novel method of conditional density estimation based on Fourier series and artificial neural networks, and compare it to other estimators on five distinct datasets. We conclude that our proposed method outperforms the other tested methods.","tags":["Nonparametric Statistics","Conditional Density Estimation","Machine Learning","Neural Networks","Deep Learning"],"title":"Conditional density estimation using Fourier series and neural networks","type":"publication"},{"authors":["F. M. da Silva","C. I. Niño","Rafael Izbicki, PhD","S. N. del. Lama\""],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"1a89b8658dd0e6a488b76d06923480c7","permalink":"http://www.rizbicki.ufscar.br/publication/water_birds/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/publication/water_birds/","section":"publication","summary":"Detecting trends in population size fluctuations is a major focus in ecology, evolution, and conservation biology. Populations of colonial waterbirds have been monitored using demographic approaches to determine annual census size (Na). We propose the addition of genetic estimates of the effective number of breeders (Nb) as indirect measures of the risk of loss of genetic diversity to improve the evaluation of demographics and increase the accuracy of trend estimates in breeding colonies. Here, we investigated which methods of the estimation of Nb are more precise under conditions of moderate genetic diversity, limited sample sizes and few microsatellite loci, as often occurs with natural populations. We used the wood stork as a model species and we offered a workflow that researchers can follow for monitoring bird breeding colonies. Our approach started with simulations using five estimators of Nb and the theoretical results were validated with empirical data collected from breeding colonies settled in the Brazilian Pantanal wetland. In parallel, we estimated census size using a corrected method based on counting active nests. Both in simulations and in natural populations, the approximate Bayesian computation (ABC) and sibship assignment (SA) methods yielded more precise estimates than the linkage disequilibrium, heterozygosity excess, and molecular coancestry methods. In particular, the ABC method performed best with few loci and small sample sizes, while the other estimators required larger sample sizes and at least 13 loci to not underestimate Nb. Moreover, according to our Nb/Na estimates (values were often ≤0.1), the wood stork colonies evaluated could be facing the loss of genetic diversity. We demonstrate that the combination of genetic and census estimates is a useful approach for monitoring natural breeding bird populations. This methodology has been recommended for populations of rare species or with a known history of population decline to support conservation efforts.","tags":["Biology","Ecology"],"title":"Considerations for monitoring population trends of colonial waterbirds using the effective number of breeders and census estimates ","type":"publication"},{"authors":null,"categories":["Blog Post"],"content":" O trabalho é de sua responsabilidade. Sempre que você precisar, estarei disponível para ajudar, mas eu não vou cobrar você. Fique atento a prazos, eles também são de sua responsabilidade. Ao escrever seu texto, sempre se coloque no papel de quem o está lendo pela primeira vez. Pense em quem você quer que consiga entender o documento (outros alunos? Professores da área específica de seu trabalho? Professores da estatística? Médicos?). Com base nisso, você terá uma ideia melhor sobre o que assumir que a pessoa já sabe e o que deve ser explicado em detalhes. Cuidado com o ponto cego do especialista: Depois de trabalhar em um projeto por algum tempo, muitos tópicos se tornam triviais para você, embora não o sejam para quem não é especialista. Pense nisso na hora de escrever o texto e apresentar o trabalho. Em particular, cuidado com notação. Seja preciso e entenda a fundo cada frase que você escrever. Obviamente, entenda muito bem seu problema e a solução que você está propondo. Cuidado com o Português/Inglês; em caso de dúvida, use o Google. Uma linguagem incorreta tira a credibilidade de seu texto. Em particular, cuidado com o uso de vírgulas: muitos erram isso frequentemente. Não separe sujeito de predicado! Ao escrever seu texto, não copie textos de outras pessoas sem as devidas referências. O ideal é sempre ler sobre o assunto, fechar os livros e só então escrever seu texto usando suas próprias palavras. Não deixe nada para a última hora. Em particular, escrever um bom texto leva muito tempo, e é escrevendo que você organiza as ideias na sua cabeça. Antes de me mandar seu documento, releia-o algumas vezes. É uma boa prática ficar pelo menos algumas horas sem olhá-lo antes de relê-lo. Não me envie um texto para que eu o revise um dia antes da entrega do trabalho. Eu levo um tempo para ler, e gosto de lê-lo novamente depois da revisão ter sido feita. Durante nossas reuniões, sempre anote o que conversamos para não esquecer. Durante uma apresentação, sempre lembre o público como o que você está falando se relaciona com o todo do trabalho. Isso ajuda a plateia a entendê-lo melhor. Coloque pouco texto no slides e ensaie muitas vezes sua apresentação. É só ensaiando que você percebe que muitas vezes é mais natural trocar a ordem de alguns slides. No dia da apresentação, leve uma cópia impressa do seu trabalho, assim como caneta e papel para anotar sugestões. Sempre, sempre faça backup de seus documentos. Recomendo usar o Dropbox, Google Drive ou Github. Sempre deixe suas pastas no computador bem organizadas e documentadas. O que é cada arquivo pode parecer óbvio agora, mas não o será daqui a um mês. Sempre dê nomes informativos aos arquivos criados (nada de novo_v3_final.R). Caso esteja fazendo/participando comigo de um projeto de pesquisa, lembre-se que atrasos afetam diretamente a mim e a outros colaboradores. Seja responsável e evite prejudicar outras pessoas e a si mesmo. ","date":1515801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"56efc3ff0451353eeda9e97ebc526d5f","permalink":"http://www.rizbicki.ufscar.br/post/dicas_alunos/","publishdate":"2018-01-13T00:00:00Z","relpermalink":"/post/dicas_alunos/","section":"post","summary":"O trabalho é de sua responsabilidade. Sempre que você precisar, estarei disponível para ajudar, mas eu não vou cobrar você. Fique atento a prazos, eles também são de sua responsabilidade.","tags":["Portuguese","Alunos"],"title":"Recomendações para meus orientandos","type":"post"},{"authors":["I. Aprahamian","E. Sassaki","M. Santos","Rafael Izbicki, PhD","R. C. Pulgrossi","M. Biella","A. C. Borges","M. Sassaki","L. Torres","I. Fernandez","O. Piao","P. Castro","P. Piao","M. S. Yassuda"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c2450b70c3893ebefd9115bac5eee2e7","permalink":"http://www.rizbicki.ufscar.br/publication/aprahamian_pulgrossi/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/aprahamian_pulgrossi/","section":"publication","summary":"The association between hypertension and frailty syndrome in older adults remains unclear. There is scarce information about the prevalence of hypertension among frail elderly patients or on its relationship with frailty. Up to one quarter of frail elderly patients present without comorbidity or disability, yet frailty is a leading cause of death. The knowledge and better control of frailty risk factors could influence prognosis. The present study evaluated: (1) the prevalence of hypertension in robust, prefrail, and frail elderly; and (2) factors that might be associated with frailty including hypertension. A cross‐sectional study was conducted in 619 older adults at a university‐based outpatient center. Study protocol included sociodemographic data, measures of blood pressure and body mass index, frailty screening according to the internationally validated FRAIL (fatigue, resistance, ambulation, illnesses, and loss of weight) scale, number of comorbidities, drug use assessment, physical activity, cognitive status, and activities of daily living. Ordinal logistic regression was used to evaluate factors associated with frailty. Prevalence of hypertension and frailty was 67.3% and 14.8%, respectively, in the total sample. Hypertension was more prevalent in the prefrail (72.5%) and frail (83%) groups than among controls (51.7%). Hypertension, physical activity, number of prescribed drugs, and cognitive performance were significantly associated with frailty status. Hypertension presented an odds ratio of 1.77 towards frailty (95% confidence interval, 1.21–2.60; P : .002). Hypertension was more prevalent in frail elderly patients and was significantly associated with frailty. Intensive control of hypertension could influence the trajectory of frailty, and this hypothesis should be explored in future prospective clinical trials.","tags":[],"title":"Hypertension is associated with frailty in older adults","type":"publication"},{"authors":["L. O. Cruz","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"72f5f0fe148655361c3bf2191a8958cd","permalink":"http://www.rizbicki.ufscar.br/publication/dengue/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/dengue/","section":"publication","summary":"","tags":["Machine Learning","Time Series"],"title":"Monitoramento online da dengue: usando o Google para predizer epidemias.","type":"publication"},{"authors":["Luís G. Esteves","Rafael Izbicki, PhD","Rafael B. Stern"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"030332e61136360dd4f72e2e5931e1f3","permalink":"http://www.rizbicki.ufscar.br/publication/2018_teaching_minimax/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/2018_teaching_minimax/","section":"publication","summary":"Teaching how to derive minimax decision rules can be challenging because of the lack of examples that are simple enough to be used in the classroom. Motivated by this challenge, we provide a new example that illustrates the use of standard techniques in the derivation of optimal decision rules under the Bayes and minimax approaches. We discuss how to predict the value of an unknown quantity, θ ∈ {0, 1}, given the opinions of n experts. An important example of such crowdsourcing problem occurs in modern cosmology, where θ indicates whether a given galaxy is merging or not, and Y1, …, Yn are the opinions from n astronomers regarding θ. We use the obtained prediction rules to discuss advantages and disadvantages of the Bayes and minimax approaches to decision theory. The material presented here is intended to be taught to first-year graduate students.","tags":["Teaching","Minimax","Crowdsourcing"],"title":"Teaching Decision Theory proof strategies using a crowdsourcing problem","type":"publication"},{"authors":["Rafael Izbicki, PhD","Ann B. Lee"],"categories":null,"content":"","date":1509840180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509840180,"objectID":"d722bcacc6fc1750a9b4f6579a2977ab","permalink":"http://www.rizbicki.ufscar.br/publication/2017_flexcode/","publishdate":"2017-11-05T00:03:00Z","relpermalink":"/publication/2017_flexcode/","section":"publication","summary":"Here we propose a fully nonparametric approach to conditional density estimation that reformulates CDE as a non-parametric orthogonal series problem where the expansion coefficients are estimated by regression. By taking such an approach, one can efficiently estimate conditional densities and not just expectations in high dimensions by drawing upon the success in high-dimensional regression. We show applications to photometric galaxy data, Twitter data, and line-of-sight velocities in a galaxy cluster.","tags":["Nonparametric Statistics","Conditional Density Estimation","High-Dimensional Inference"],"title":"Converting High-Dimensional Regression to High-Dimensional Conditional Density Estimation","type":"publication"},{"authors":["P.E. Freeman","Rafael Izbicki, PhD","A.B. Lee"],"categories":null,"content":"","date":1509321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509321600,"objectID":"a26a9a8a49dddc1a5aefd6fa801a3868","permalink":"http://www.rizbicki.ufscar.br/publication/mnras_bias/","publishdate":"2017-10-30T00:00:00Z","relpermalink":"/publication/mnras_bias/","section":"publication","summary":"Photometric redshift estimation is an indispensable tool of precision cosmology. One problem that plagues the use of this tool in the era of large-scale sky surveys is that the bright galaxies that are selected for spectroscopic observation do not have properties that match those of (far more numerous) dimmer galaxies; thus, ill-designed empirical methods that produce accurate and precise redshift estimates for the former generally will not produce good estimates for the latter. In this paper, we provide a principled framework for generating conditional density estimates (i.e. photometric redshift PDFs) that takes into account selection bias and the covariate shift that this bias induces. We base our approach on the assumption that the probability that astronomers label a galaxy (i.e. determine its spectroscopic redshift) depends only on its measured (photometric and perhaps other) properties x and not on its true redshift. With this assumption, we can explicitly write down risk functions that allow us to both tune and compare methods for estimating importance weights (i.e. the ratio of densities of unlabelled and labelled galaxies for different values of x) and conditional densities. We also provide a method for combining multiple conditional density estimates for the same galaxy into a single estimate with better properties. We apply our risk functions to an analysis of ≈106 galaxies, mostly observed by Sloan Digital Sky Survey, and demonstrate through multiple diagnostic tests that our method achieves good conditional density estimates for the unlabelled galaxies.","tags":["Nonparametric Statistics","Density Ratio","Selection Bias","Astrostatistics","Photometric Redshift Prediction"],"title":"A unified framework for constructing, tuning and assessing photometric redshift density estimates in a selection bias setting","type":"publication"},{"authors":["V. Fossaluza","Rafael Izbicki, PhD","G. M. Silva","L. G. Esteves"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"dacdaf5b8b55444c4aff9f968b0f40ee","permalink":"http://www.rizbicki.ufscar.br/publication/tas_coherent/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/tas_coherent/","section":"publication","summary":"Multiple hypothesis testing, an important quantitative tool to report the results of scientific inquiries, frequently leads to contradictory conclusions. For instance, in an analysis of variance (ANOVA) setting, the same dataset can lead one to reject the equality of two means, say mu1 : mu2, but at the same time to not reject the hypothesis that mu1 : mu2 : 0. These two conclusions violate the coherence principle introduced by Gabriel in 1969, and lead to results that are difficult to communicate, and, many times, embarrassing for practitioners of statistical methods. Although this situation is common in the daily life of statisticians, it is usually not discussed in courses of statistics. In this work, we enrich the teaching and discussion of this important topic by investigating through a few examples whether several standard test procedures are coherent or not. We also discuss the relationship between coherent tests and measures of support. Finally, we show how a Bayesian decision-theoretical framework can be used to build coherent tests. These approaches to coherence enlighten when such property is appealing in multiple testing and provide means of obtaining it.","tags":["Hypothesis Tests","Logical Consistency"],"title":"Coherent hypothesis testing","type":"publication"},{"authors":["Rafael Izbicki, PhD","Joseph B. Kadane"],"categories":null,"content":" ","date":1496322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496322000,"objectID":"30783a0ad51abefd3f9bf6bfe83b0f9a","permalink":"http://www.rizbicki.ufscar.br/talk/a-model-for-the-evolution-of-languages/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/a-model-for-the-evolution-of-languages/","section":"event","summary":"I'll present a model for infering the evolution of the phonology of languages. A relevant innovation of this model is that it captures the regularity of sound changes.","tags":["Phylology","Historical Linguistics","Computational Linguistics"],"title":"A model for the evolution of languages","type":"event"},{"authors":["Rafael Izbicki, PhD"],"categories":null,"content":" ","date":1496322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496322000,"objectID":"9f48adbb9ae39b0225fca6ab56a86e42","permalink":"http://www.rizbicki.ufscar.br/talk/teoria-da-decisao-e-aplicacoes-ao-direito/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/teoria-da-decisao-e-aplicacoes-ao-direito/","section":"event","summary":"Introdução à Teoria da Decisão com aplicações na Jurimetria. Em particular, discute-se o efeito da ausência de taxas judiciais nos JEC's sobre o valor pedido pelo autor.","tags":["Decision Theory","Jurimetrics","JEC"],"title":"Teoria da decisão e aplicações ao Direito","type":"event"},{"authors":["Julio M. Stern","Rafael Izbicki, PhD","Luís G. Esteves","Rafael B. Stern"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"cefe97fae2649b1cf06059834b282336","permalink":"http://www.rizbicki.ufscar.br/publication/2017_hexagon/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/2017_hexagon/","section":"publication","summary":"Although logical consistency is desirable in scientific research, standard statistical hypothesis tests are typically logically inconsistent. In order to address this issue, previous work introduced agnostic hypothesis tests and proved that they can be logically consistent while retaining statistical optimality properties. This paper characterizes the credal modalities in agnostic hypothesis tests and uses the hexagon of oppositions to explain the logical relations between these modalities. Geometric solids that are composed of hexagons of oppositions illustrate the conditions for these modalities to be logically consistent. Prisms composed of hexagons of oppositions show how the credal modalities obtained from two agnostic tests vary according to their threshold values. Nested hexagons of oppositions summarize logical relations between the credal modalities in these tests and prove new relations.","tags":["Hypothesis Test","Agnostic Test","Logical Coherence"],"title":"Logically-consistent hypothesis testing and the hexagon of oppositions","type":"publication"},{"authors":["Afonso F. Vaz","Rafael Izbicki, PhD","Rafael B. Stern"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"5d88d16f07786c4c898a5c81de185484","permalink":"http://www.rizbicki.ufscar.br/publication/2017_quantification_maxent/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_quantification_maxent/","section":"publication","summary":"Several machine learning applications use classifiers as a way of quantifying the prevalence of positive class labels in a target dataset, a task named quantification. For instance, a naive a way of determining what proportion of people like a given product with no labeled reviews is to (i) train a classifier based on the Google Shopping reviews to predict whether a user likes a product given its review, and then (ii) apply this classifier to Facebook/Google+ posts about that product. It is well known that such a two-step approach, named Classify and Count, fails because of dataset shift, and thus, several improvements have been recently proposed under an assumption named prior shift. Unfortunately, these methods only explore the relationship between the covariates and the response via classifiers. Moreover, the literature lacks in the theoretical foundation to improve these techniques. We propose a new family of estimators named Ratio Estimator which is able to explore the relationship between the cov ariates and the response using any function g:X→R and not only classifiers. We show that for some choices of g, our estimator matches standard estimators used in the literature. We also explore alternative ways of constructing functions g that lead to estimators with good performance, and compare them using real datasets. Finally, we provide a theoretical analysis of the method.","tags":["Quantification","Prior Shift","Ratio Estimator"],"title":"Prior Shift Using the Ratio Estimator","type":"publication"},{"authors":["Rafael Izbicki, PhD","Ann B. Lee","Peter E. Freeman"],"categories":null,"content":"","date":1486252980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486252980,"objectID":"fcd00fab3a3fd6028bbc81e5679d40d9","permalink":"http://www.rizbicki.ufscar.br/publication/2017_photo_z/","publishdate":"2017-02-05T00:03:00Z","relpermalink":"/publication/2017_photo_z/","section":"publication","summary":"We describe a general framework for properly constructing and assessing nonparametric conditional density estimators under selection bias, and for combining two or more estimators for optimal performance. This leads to new improved photo-z estimators. We illustrate our methods on data from the Sloan Data Sky Survey and an application to galaxy-galaxy lensing.","tags":["Nonparametric Statistics","Density Ratio","Selection Bias"],"title":"Photo-z estimation: An example of nonparametric conditional density estimation under selection bias","type":"publication"},{"authors":["I. Aprahamian","N.O.C. Cezar","Rafael Izbicki, PhD","S.M. Lin","D.L.V. Paulo","A. Fattori","M. M. Biella","W. Jacob Filho","M.S. Yassuda"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"5ef265a02c228d83c5fcdc84d7760953","permalink":"http://www.rizbicki.ufscar.br/publication/aprahamian_frail/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/aprahamian_frail/","section":"publication","summary":"","tags":[],"title":" Screening for frailty with the FRAIL scale: a comparison with the phenotype criteria","type":"publication"},{"authors":["N.O.C. Cezar","Rafael Izbicki, PhD","D. Cardoso","G.C. Almeida","L. Valiengo","M.V.Z. Camargo","V.F. Orestes","I. Aprahamian","M. S. Yassuda"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"0ca39392b51f88d023bb922f9adfe43d","permalink":"http://www.rizbicki.ufscar.br/publication/aprahamian_frail2/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/aprahamian_frail2/","section":"publication","summary":"","tags":[],"title":"Frailty in older adults with aMCI due to AD: a comparison of two models of frailty characterization","type":"publication"},{"authors":["P. Ianishi","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"4add4aab239a1bcb6f458c4385f55e69","permalink":"http://www.rizbicki.ufscar.br/publication/desbalanceados/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/desbalanceados/","section":"publication","summary":"","tags":["Machine Learning"],"title":"Classificação morfológica de galáxias em conjuntos de dados desbalanceados.","type":"publication"},{"authors":["Rafael Izbicki, PhD","A. B. Lee"],"categories":null,"content":"","date":1478044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478044800,"objectID":"cc71d2f9b2038886bc16f72980081df5","permalink":"http://www.rizbicki.ufscar.br/publication/spec_series_cde/","publishdate":"2016-11-02T00:00:00Z","relpermalink":"/publication/spec_series_cde/","section":"publication","summary":"In some applications (e.g., in cosmology and economics), the regression E[Z|x] is not adequate to represent the association between a predictor x and a response Z because of multi-modality and asymmetry of f(z|x); using the full density instead of a single-point estimate can then lead to less bias in subsequent analysis. As of now, there are no effective ways of estimating f(z|x) when x represents high-dimensional, complex data. In this article, we propose a new nonparametric estimator of f(z|x) that adapts to sparse (low-dimensional) structure in x. By directly expanding f(z|x) in the eigenfunctions of a kernel-based operator, we avoid tensor products in high dimensions as well as ratios of estimated densities. Our basis functions are orthogonal with respect to the underlying data distribution, allowing fast implementation and tuning of parameters. We derive rates of convergence and show that the method adapts to the intrinsic dimension of the data. We also demonstrate the effectiveness of the series method on images, spectra, and an application to photometric redshift estimation of galaxies. ","tags":["Nonparametric Statistics","Conditional Density Estimation","High-Dimensional Inference"],"title":"Nonparametric Conditional Density Estimation in a High-Dimensional Regression Setting.","type":"publication"},{"authors":["A. B. Lee","Rafael Izbicki, PhD"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"7cfe60aebf040479c065adaed2afc823","permalink":"http://www.rizbicki.ufscar.br/publication/spec_series_reg/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/publication/spec_series_reg/","section":"publication","summary":"A key question in modern statistics is how to make fast and reliable inferences for complex, high-dimensional data. While there has been much interest in sparse techniques, current methods do not generalize well to data with nonlinear structure. In this work, we present an orthogonal series estimator for predictors that are complex aggregate objects, such as natural images, galaxy spectra, trajectories, and movies. Our series approach ties together ideas from manifold learning, kernel machine learning, and Fourier methods. We expand the unknown regression on the data in terms of the eigenfunctions of a kernel-based operator, and we take advantage of orthogonality of the basis with respect to the underlying data distribution, P, to speed up computations and tuning of parameters. If the kernel is appropriately chosen, then the eigenfunctions adapt to the intrinsic geometry and dimension of the data. We provide theoretical guarantees for a radial kernel with varying bandwidth, and we relate smoothness of the regression function with respect to P to sparsity in the eigenbasis. Finally, using simulated and real-world data, we systematically compare the performance of the spectral series approach with classical kernel smoothing, k-nearest neighbors regression, kernel ridge regression, and state-of-the-art manifold and local regression methods.","tags":["Nonparametric Statistics","Regression","High-Dimensional Inference"],"title":"A Spectral Series Approach to High-Dimensional Nonparametric Regression.","type":"publication"},{"authors":["Luís G. Esteves","Rafael Izbicki, PhD","Julio M. Stern","Rafael B. Stern"],"categories":null,"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"d91ab9df4cafea5203895375d8b23d54","permalink":"http://www.rizbicki.ufscar.br/publication/2016_agnostic/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/publication/2016_agnostic/","section":"publication","summary":"Simultaneous hypothesis tests can fail to provide results that meet logical requirements. For example, A implies B, there exist tests that, based on the same data, reject B but not A. This paper develops logically-coherent agnostic tests that also perform well statistically. These tests are characterized as region estimator-based tests.","tags":["Hypothesis Test","Logical Coherence","Agnostic Test"],"title":"The Logical Consistency of Simultaneous Agnostic Hypothesis Tests","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ddbc7f3661c1cbb74424968308d493a0","permalink":"http://www.rizbicki.ufscar.br/resource/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/resource/example/","section":"resource","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"resource"},{"authors":["G. M. Silva","L. G. Esteves","V. Fossaluza","Rafael Izbicki, PhD","S. Wechsler"],"categories":null,"content":"","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"60c75ee772850e4ed1bfe43a1ba94d82","permalink":"http://www.rizbicki.ufscar.br/publication/entropy_bayes_consist/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/publication/entropy_bayes_consist/","section":"publication","summary":"This work addresses an important issue regarding the performance of simultaneous test procedures: the construction of multiple tests that at the same time are optimal from a statistical perspective and that also yield logically-consistent results that are easy to communicate to practitioners of statistical methods. For instance, if hypothesis A implies hypothesis B, is it possible to create optimal testing procedures that reject A whenever they reject B? Unfortunately, several standard testing procedures fail in having such logical consistency. Although this has been deeply investigated under a frequentist perspective, the literature lacks analyses under a Bayesian paradigm. In this work, we contribute to the discussion by investigating three rational relationships under a Bayesian decision-theoretic standpoint: coherence, invertibility and union consonance. We characterize and illustrate through simple examples optimal Bayes tests that fulfill each of these requisites separately. We also explore how far one can go by putting these requirements together. We show that although fairly intuitive tests satisfy both coherence and invertibility, no Bayesian testing scheme meets the desiderata as a whole, strengthening the understanding that logical consistency cannot be combined with statistical optimality in general. Finally, we associate Bayesian hypothesis testing with Bayes point estimation procedures. We prove the performance of logically-consistent hypothesis testing by means of a Bayes point estimator to be optimal only under very restrictive conditions.","tags":["Hypothesis Tests","Logical Consistency","Foundations of Statistics"],"title":"A Bayesian Decision-Theoretic Approach to Logically-Consistent Hypothesis Testing","type":"publication"},{"authors":["L. G. Esteves","Rafael Izbicki, PhD","J. M. Stern","R. B. Stern"],"categories":null,"content":"","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"41061b82cbfac3d8ccd716dd7e9a64c7","permalink":"http://www.rizbicki.ufscar.br/publication/entropy_consist/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/publication/entropy_consist/","section":"publication","summary":"Simultaneous hypothesis tests can fail to provide results that meet logical requirements. For example, if A and B are two statements such that A implies B, there exist tests that, based on the same data, reject B but not A. Such outcomes are generally inconvenient to statisticians (who want to communicate the results to practitioners in a simple fashion) and non-statisticians (confused by conflicting pieces of information). Based on this inconvenience, one might want to use tests that satisfy logical requirements. However, Izbicki and Esteves shows that the only tests that are in accordance with three logical requirements (monotonicity, invertibility and consonance) are trivial tests based on point estimation, which generally lack statistical optimality. As a possible solution to this dilemma, this paper adapts the above logical requirements to agnostic tests, in which one can accept, reject or remain agnostic with respect to a given hypothesis. Each of the logical requirements is characterized in terms of a Bayesian decision theoretic perspective. Contrary to the results obtained for regular hypothesis tests, there exist agnostic tests that satisfy all logical requirements and also perform well statistically. In particular, agnostic tests that fulfill all logical requirements are characterized as region estimator-based tests. Examples of such tests are provided.","tags":["Hypothesis Tests","Logical Consistency","Foundations of Statistics"],"title":"The Logical Consistency of Simultaneous Agnostic Hypothesis Tests","type":"publication"},{"authors":["J. F. Cecato","J. E. Martinelli","Rafael Izbicki, PhD","M. S. Yassuda","I. Aprahamian"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"fc5ca054c46d0bc76570678443d2c3c4","permalink":"http://www.rizbicki.ufscar.br/publication/aprahamian_moca/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/aprahamian_moca/","section":"publication","summary":"","tags":[],"title":"A subtest analysis of the Montreal cognitive assessment (MoCA): which subtests can best discriminate between healthy controls, mild cognitive impairment and Alzheimer’s disease?","type":"publication"},{"authors":["Rafael Izbicki, PhD","L. G. Esteves"],"categories":null,"content":"","date":1425168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425168000,"objectID":"e38b7e4709ca3ccdd83b1aa2c4c622df","permalink":"http://www.rizbicki.ufscar.br/publication/igpl_consist/","publishdate":"2015-03-01T00:00:00Z","relpermalink":"/publication/igpl_consist/","section":"publication","summary":"Many authors have argued that, when performing simultaneous statistical test procedures, one should seek for solutions that lead to decisions that are consistent and, consequently, easier to communicate to practitioners of statistical methods. In this way, the set of hypotheses that are rejected and the set of hypotheses that are not rejected by a testing procedure should be consistent from a logic standpoint. For instance, if hypothesis  A  implies hypothesis  B, a procedure that rejects  B  should also reject  A, a property not always met by multiple test procedures. We contribute to this discussion by exploring how far one can go in constructing coherent procedures while still preserving statistical optimality. This is done by studying four types of logical consistency relations. We show that although the only procedures that satisfy more than (any) two of these properties are simple tests based on point estimation, it is possible to construct various interesting methods that fulfil one or two of them while preserving different statistical optimality criteria. This is illustrated with several Bayesian and frequentist examples. We also characterize some of these properties under a decision-theoretic framework.","tags":["Hypothesis Tests","Logical Consistency"],"title":"Logical consistency in simultaneous statistical test procedures","type":"publication"},{"authors":["Rafael Izbicki, PhD","A. B. Lee","C. M. Schafer"],"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"a6c5de308311922f3b02930c0de9b780","permalink":"http://www.rizbicki.ufscar.br/publication/aistats_2014/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/publication/aistats_2014/","section":"publication","summary":"The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification. Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-toimplement, fully nonparametric density ratio estimator that expands the ratio in terms of the eigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step. We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be wellapproximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments.","tags":["Machine Learning","Conference","Nonparametric Statistics","Density Ratio","Likelihood Estimation","ABC"],"title":"High-Dimensional density ratio estimation with extensions to approximate likelihood computation","type":"publication"},{"authors":["P. E. Freeman","Rafael Izbicki, PhD","A. B.  Lee","J.A. Newman","C. J. Conselice","A.M. Koekemoer","J.M. Lotz","M. Mozena"],"categories":null,"content":"","date":1385856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385856000,"objectID":"fe4f2e6ddb52476811e4a0b4d00b5a1f","permalink":"http://www.rizbicki.ufscar.br/publication/mnras_mergers/","publishdate":"2013-12-01T00:00:00Z","relpermalink":"/publication/mnras_mergers/","section":"publication","summary":"","tags":["Machine Learning","Astrostatistics"],"title":"New image statistics for detecting disturbed galaxy morphologies at high redshift","type":"publication"},{"authors":["S. Wechsler","Rafael Izbicki, PhD","L. G. Esteves"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383264000,"objectID":"38e77c3ea965e4446046ee110ff75b59","permalink":"http://www.rizbicki.ufscar.br/publication/tas_identif/","publishdate":"2013-11-01T00:00:00Z","relpermalink":"/publication/tas_identif/","section":"publication","summary":"This article discusses the concept of identifiability in simple probability calculus. Emphasis is given to Bayesian solutions. In particular, we compare Bayes and maximum likelihood estimators. We advocate adoption of informative prior probabilities for the Bayesian operation in place of diffuse or reference priors. We also discuss the concept of identifying functions.","tags":["Identifiability","Bayesian Statistics"],"title":"A Bayesian Look at Nonidentifiability: a Simple Example","type":"publication"},{"authors":["F. C. Gomes","A. G. Andrade","Rafael Izbicki, PhD","A. Moreira-Almeida","L. G. Oliveira"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383264000,"objectID":"b5f60b740767d00bf8104211798f0044","permalink":"http://www.rizbicki.ufscar.br/publication/gomes/","publishdate":"2013-11-01T00:00:00Z","relpermalink":"/publication/gomes/","section":"publication","summary":"","tags":[],"title":"Religion as a protective factor against drug use among Brazilian university students: a national survey","type":"publication"},{"authors":["Rafael Izbicki, PhD","Rafael B. Stern"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"a727f0a7e6e088741ca24a54404e54d3","permalink":"http://www.rizbicki.ufscar.br/publication/2013_crowdsourcing/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/2013_crowdsourcing/","section":"publication","summary":"Experts classifying data are often imprecise. Recently, several models have been proposed to train classifiers using the noisy labels generated by these experts. How to choose between these models? In such situations, the true labels are unavailable. Thus, one cannot perform model selection using the standard versions of methods such as empirical risk minimization and cross validation. In order to allow model selection, we present a surrogate loss and provide theoretical guarantees that assure its consistency. Next, we discuss how this loss can be used to tune a penalization which introduces sparsity in the parameters of a traditional class of models. Sparsity provides more parsimonious models and can avoid overfitting. Nevertheless, it has seldom been discussed in the context of noisy labels owing to the difficulty in model selection and, therefore, in choosing tuning parameters. We apply these techniques to several sets of simulated and real data.","tags":["Crowdsourcing","Machine Learning","EM"],"title":"Learning with many experts: Model selection and sparsity","type":"publication"},{"authors":["Rafael Izbicki, PhD","V. Fossaluza","A. G. Hounie","E. Y. Nakano","C. A. de B. Pereira"],"categories":["paper"],"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"2f950a9968329753be27edc22887c3db","permalink":"http://www.rizbicki.ufscar.br/publication/nested/","publishdate":"2012-11-01T00:00:00Z","relpermalink":"/publication/nested/","section":"publication","summary":"","tags":["Hypothesis Tests","Genetics","FBST","Bayesian Statistics"],"title":"Testing Allele Homogeneity: The Problem of Nested Hypotheses","type":"publication"},{"authors":["H. Brentani","E. Y. Nakano","C. B. Martins","Rafael Izbicki, PhD","C. A. de B. Pereira"],"categories":null,"content":"","date":1320105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1320105600,"objectID":"7e0e8d15abbdaf4a559b25ece2a47185","permalink":"http://www.rizbicki.ufscar.br/publication/disequilibrium/","publishdate":"2011-11-01T00:00:00Z","relpermalink":"/publication/disequilibrium/","section":"publication","summary":"","tags":["FBST","Bayesian Inference","Hypothesis Tests","Genetics"],"title":"Disequilibrium Coefficient: A Bayesian Perspective","type":"publication"},{"authors":["R. Barcelos-Ferreira","Rafael Izbicki, PhD","D. C. Steffens","C. M. C. Bottino"],"categories":null,"content":"","date":1288569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1288569600,"objectID":"310553469bfa9fdc3488031d3a50f981","permalink":"http://www.rizbicki.ufscar.br/publication/meta_analysis/","publishdate":"2010-11-01T00:00:00Z","relpermalink":"/publication/meta_analysis/","section":"publication","summary":"","tags":[],"title":"Depressive morbidity and gender in community-dwelling Brazilian elderly: systematic review and meta-analysis","type":"publication"},{"authors":["I. Aprahamian","J. E. Martinelli","J. Cecato","Rafael Izbicki, PhD","M. S. Yassuda"],"categories":null,"content":"","date":1285891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1285891200,"objectID":"674279cc36075727d8d4c44fa36793ef","permalink":"http://www.rizbicki.ufscar.br/publication/aprahamian_camcog/","publishdate":"2010-10-01T00:00:00Z","relpermalink":"/publication/aprahamian_camcog/","section":"publication","summary":"","tags":[],"title":"Can the CAMCOG be a good cognitive test for patients with Alzheimer’s disease with low levels of education?","type":"publication"},{"authors":["I. Aprahamian","B. S. Diniz","Rafael Izbicki, PhD","M. Radanovic","P. V. Nunes","O. V. Forlenza"],"categories":null,"content":"","date":1285891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1285891200,"objectID":"40a176570dc2265df2b929e7cf4d9423","permalink":"http://www.rizbicki.ufscar.br/publication/aprahamian_camcog2/","publishdate":"2010-10-01T00:00:00Z","relpermalink":"/publication/aprahamian_camcog2/","section":"publication","summary":"","tags":[],"title":"Optimizing the CAMCOG test in the screening for mild cognitive impairment and incipient dementia: saving time with relevant domains","type":"publication"}]