---
title: "Aula 2 - Medidas resumo"
author: "Rafael Izbicki e Rafael Bassi Stern"
date: '2018-08-16'
bibliography: ipaee.bib
output: 
  html_document:
    toc: true
    number_sections: true
editor_options: 
  chunk_output_type: console
---

# Medidas resumo

Os resultados de um experimento geralmente apresentam variabilidade.
Esta variabilidade pode acontecer, por exemplo, 
por falta de controle nas condições experimentais ou
por erros de medição.
Assim, se obtivermos um número elevado de observações,
pode ser difícil obter informações relevantes 
meramente olhando para o banco de dados.
Por exemplo, os números a seguir são 
os comprimento das pétalas nas 150 observações
do banco de dados *Iris* [@Anderson1936].
O que você consegue observar?

```{r}
 data(iris)
 print(iris$Petal.Length)
```

Ao invés da inspeção direta de uma variável,
podemos resumí-la em números que 
expressam algumas de suas características.
A seguir, estudaremos algumas destas
medidas de resumo.

## Medidas de centralidade

Uma medida de centralidade descreve um número ao
redor dos quais as observações se concentram.
Ela expressa um valor ``típico'' nas observações
para uma determinada variável.
Existem várias possíveis medidas de centralidade,
algumas das quais veremos a seguir.

### Média

A **média** de uma variável,
comumente designada por $\bar{X}$,
é obtida somando
todas as observações desta e
dividindo o resultado pelo 
número total de observações.
Este procedimento é sintetizado
da seguinte forma:

$$
\bar{X} = \frac{\sum_{i=1}^{n}{X_i}}{n} = \frac{X_1 + \ldots + X_n}{n}
$$
Nesta expressão, o termo 
$\sum_{i=1}^{n}{X_i}$ é traduzido como
"em $X_i$ substituta $i$ por 
todos os números entre $1$ e $n$ e
some os valores encontrados".
Em particular, o símbolo "$\sum$" é
chamado de somatória.

No **R**, a média pode ser calculada
usando o comando *mean()*.
Por exemplo, a média do comprimento de pétalas
para a amostra de flores do gênero iris
pode ser calculada da seguinte forma:

```{r}
mean(iris$Petal.Length)
```

### Mediana

A **mediana** de uma variável é
um número tal que há o mesmo número de
observações maiores e menores do que ele.
No **R**, a mediana é 
calculada pela função *median()*.


```{r}
median(iris$Petal.Length)
```

A mediana é menos afetada por valores extremos
do que a média. Por isso, é comum dizer que
a mediana é uma medida **robusta**.
Este conceito é ilustrado a seguir.

```{r}
 dados = c(0, 0.1, 0.1, 0.2, 0.25, 0.5, 0.7, 0.9, 1.1, 10000)
 c(mean(dados), median(dados))
```

Observamos que, dos 10 dados,
9 estão concentrados próximo a 0 e
1 tem o valor 10.000.
Enquanto que a média de aproximadamente 1.000
é afetada pelo valor extremo,
a mediana de 0.375 não o é.
É comum chamarmos observações atípicas,
como o valor 10.000 neste caso,
de **outliers**. 

*Observação*: Note que como no exemplo acima
existe um número par de dados, 
a mediana foi tomada como a média entre
0.25 e 0.5, as observações 5 e 6
em ordem crescente.

### Moda

A **moda** é o valor mais frequente observado nos dados.
Como em variáveis contínuas tipicamente 
não observamos valores repetidos,
a moda não é usado nestes casos.
Por outro lado, dentre média, mediana e moda,
a moda é a única medida resumo que pode ser 
aplicada a variáveis nominais.
Considere que observamos os dados:
azul, azul, azul, vermelho, verde, verde.
Observamos as cores azul, vermelho e verde
respectivamente, 3, 1 e 2 vezes.
Portanto, a cor azul é a mais frequente,
sendo a moda desta variável.

## Medidas de variabilidade

Medidas de variabilidade indicam 
o quanto as observações variam ao 
redor da medida de centralidade.
Em outras palavras, indicam
o quão longe podemos esperar que
uma observação esteja do valor
típico para aquela variável.
Existem diversas medidas de variabilidade,
algumas das quais apresentamos a seguir.

### Amplitude

A amplitude é a diferença entre
o maior e o menor valor observado.
Esta medida de variabilidade é
fortemente influenciada por valores extremos
nas observações, como outliers.
O exemplo a seguir calcula a amplitude
do comprimento das sépalas no 
banco de dados *iris*.

```{r}
 max(iris$Sepal.Length) - min(iris$Sepal.Length)
```

### Variância e desvio padrão

Intuitivamente, podemos imaginar uma
medida de variabilidade que calcule 
a média do quanto os dados desviam do centro.
Se tomarmos como centro das observações a média,
então podemos pensar no desvio da i-ésima observação 
como $D_i = X_i-\bar{X}$.
Contudo, esta medida de desvio apresenta um problema.
Por exemplo, considere os dados: 0, 10, 20.
A média das observações é 10 e os desvios são: -10, 0, 10.
Assim, se tomarmos a média dos desvios obteremos o valor 0.
O problema é que, ainda que o desvio de 0 e 20 sejam -10 e 10,
estas observações estão igualmente distantes da média.
Para corrigir este problema, 
podemos tomar a média dos desvios ao quadrado, isto é,
a média de $D_i^2 = (X_i-\bar{X})^2$.
No exemplo apresentado, os desvios ao quadrado são 100, 0 e 100
e a média destes valores é $\frac{200}{3}$.
Neste caso, as observações -10 e 10 contribuem igualmente
para a variabilidade dos dados em relação à média.
Formalmente a **variância**, $S^2$, é definida como:

$$
 S^2 = \frac{\sum_{i=1}^{n}{(X_i-\bar{X})^2}}{n}
$$

Note que a variância não está na mesma escala das observações.
Quando os desvios são elevados ao quadrado,
a unidade de medida é alterada para o quadrado
da unidade de medida original.
Assim, para obter uma medida mais interpretável
de varibilidade, é comum tomar
a raiz quadrada da variância.
Esta medida é chamada de desvio padrão, $S$,
e é definida como:

$$
S = \sqrt{S^2}
$$

A variância e o desvio padrão
para o comprimento das sépalas
é calculado no **R** da seguinte forma:

```{r}
c(var(iris$Sepal.Length), sd(iris$Sepal.Length))
```

Para muitos tipos de dado, é comum que
as observações se concentrem num intervalo de
2 desvios padrão para cada lado da média.
Isto é, é comum que a maior parte das observações
esteja no intervalo $[\bar{X}-2S,\bar{X}+2S]$.
Neste sentido, este intervalo indica a faixa
de observações tipicamente observadas.
A formalização deste raciocínio será estudada
em aulas futuras.
No exemplo do comprimento das sépalas,
obtemos o intervalo $[2.7,6.0]$.

### Intervalo interquartílico

O percentil de ordem $p$ de uma variável
é um número tal que a quantidade de observações
menores e maiores do que este número segue
a proporção $p$ e $1-p$.
Por exemplo, a mediana é o percentil de ordem $0.5$.
Dada a sua importância,
os percentis de ordem 0.25, 0.5 e 0.75
também são chamados de 
$1^o$, $2^o$ e $3^o$ quartis.
No **R**, é possível obter o percentil de ordem $p$
usando o comando *quantile(dados, p)*.
Este comando para os percentis de ordem
0.25, 0.5 e 0.75 para o comprimento de sépalas 
é ilustrado a seguir:

```{r}
quantile(iris$Sepal.Length, c(0.25, 0.5, 0.75))
```

Por construção, aproximadamente metade dos dados estão
entre o $1^o$ e o $3^o$ quartil, isto é,
este também pode ser interpretado como 
um intervalo de valores tipicamente assumidos pelas observações.
Por exemplo, no caso do comprimento das sépalas, 
obtemos o intervalo $[5.1, 6.4]$.

Alternativamente, podemos construir um
intervalo mais conservador exigindo que, por exemplo,
$95%$ das observações estejam dentro dele.
Este intervalo é obtido tomando os valores entre
o percentil $0.025$ e $0.975$.
No caso do comprimento das sépalas, obtemos o intervalo:

```{r}
quantile(iris$Sepal.Length, c(0.025, 0.975))
```

O tamanho da região em que as observações tipicamente caem
é uma medida alternativa de variabilidade.
Especificamente, a subtraindo o $1^o$ quartil
do $3^o$ quartil obtém-se a medida chamada de 
**intervalo interquartílico**.
No caso do comprimento das sépalas,
o intervalo interquartílico é 
$6.4-5.1=1.3$.

## Medidas de associação

### Regressão Linear e Método de Mínimos Quadrados

Frequentemente, estamos interessados
em verificar a associação entre duas
variáveis.
Para o caso de variáveis contínuas $X$ e $Y$, uma forma
de se fazer isso é via uma *regressão linear*.
Nela assumimos que
$$y_i \approx \beta_0 + \beta_1 x_i,$$
isto é, assumimos que a relação entre as duas variáveis pode ser
bem aproximada por uma reta. 
Utilizando os dados, buscamos então por valores $(\beta_0,\beta_1)$
que descrevam bem essa relação. Um critério para encontrar esses valores é o método de mínimos quadrados, que consistem em buscar
$(\beta_0,\beta_1)$ que minimizam
$$\frac{1}{n}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2.$$

No R, esses valores podem ser encontrados utilizando-se a função lm:
```{r}
ajuste <- lm("Petal.Length~Sepal.Length",data = iris)
coeficientes <- coefficients(ajuste)
print(coeficientes)
```

Podemos agora adicionar essas a reta ajusta ao gráfico 
de dispersão:

```{r}
plot(iris$Sepal.Length,iris$Petal.Length)
abline(a=coeficientes[1],b=coeficientes[2],col=2)
```


### Correlação Linear

O coeficiente $\beta_1$ de uma regressão linear, o coeficiente
angular da reta, mede a associação entre $Y$ e $X$: valores 
altos de $|\beta_1|$ indicam forte associação, enquanto que valores próximos
a zero indicam fraca associação. 

Frequentemente temos interesse em verificar a associação entre
diversos pares de variáveis. 
Por exemplo: o que está mais associado com a expectativa de vida
de um indivíduo? Índice de massa corpórea ou renda familiar?
Uma forma de responder essa pergunta é: (i) fazer uma regressão
da expectativa de vida no índice de massa corpórea e obter o 
coeficiente angular dessa relação e (ii)
fazer uma regressão
da expectativa de vida na renda familiar e obter o 
coeficiente angular dessa relação. 
Contudo, como $\beta_1$ depende da escala destas variáveis,
comparar os valores numéricos desses coeficientes não é trivial.

Um forma de contornar esse problema é *padronizar* tanto $X$ quanto
$Y$ **antes** de se aplicar a regressão linear. Isto é, pode-se
fazer 
$$X_i \gets X_i/S_X$$
e
$$Y_i \gets Y_i/S_Y,$$
em que $S_X$ é o desvio padrão de $X$ e 
$S_Y$ é o desvio padrão de $Y$. Pode-se então ajustar
a regressão linear entre essas duas novas variáveis. 

O coeficiente angular ajustado desta forma é chamado de
*correlação de Pearson*, e ele possui uma fórmula simples:
$$r =\frac{\sum ^n _{i=1}(X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum ^n _{i=1}(X_i - \bar{X})^2} \sqrt{\sum ^n _{i=1}(Y_i - \bar{Y})^2}}$$
A correlação de Pearson sempre está entre -1 e 1.
Uma correlação próxima de 1 indica forte associação linear positiva,
enquanto que uma correlação próxima de -1 indica forte 
associação linear negativa. Correlações próximas a zero indicam
ausência de associação linear.

Assim, não é necessário fazer a padronização das variáveis para calcular
o coeficiente de correlação. No R, isso pode ser feito usando a função *cor*.

```{r}
plot(iris$Sepal.Length,iris$Sepal.Width)
cor(iris$Sepal.Length,iris$Sepal.Width)
```

```{r}
plot(iris$Sepal.Length,iris$Petal.Length)
cor(iris$Sepal.Length,iris$Petal.Length)
```


## Resumo de observações no R

No R, é possível obter diversas 
medidas resumo usando a função *summary()*.
Por exemplo, no caso do comprimento de sépalas,
obtemos:

```{r}
summary(iris$Sepal.Length)
```

# Exercícios

1. No banco de dados *iris*, calcule medidas resumo
para o comprimento e largura das pétalas e para
a largura das sépalas.

2. Na definição da variância, usamos
a média dos desvios quadrado. Esta é
uma possível maneira de fazer com que
desvios negativos e positivos fossem tratados como iguais.
Você consegue pensar em outra forma
de eliminar o sinal do desvio que 
não elevando-o ao quadrado?

3. Para cada espécie no banco de dados *iris*,
obtenha uma medida de centralidade e
uma de variabilidade para o comprimento das sépalas.

4. O comprimento das sépalas da espécie
*Iris setosa* é consideravelmente menor que
o da espécie *Iris versicolor* que, por sua vez,
é menor do que o da espécie *Iris virginica*.
Considere que em um banco de dados temos
$150$ flores da espécie *Iris setosa* e, em outro,
temos 50 exemplares de cada espécie.
Qual banco de dados terá maior variabilidade
em relação ao comprimento das sépalas?

5. Qual é a variável contínua mais correlacionada com
Sepal.Lenght? Faça um gráfico de dispersão
entre essa variável e Sepal.Lenght, adicionando no gráfico
a reta que melhor descreve a relação entre essas variáveis.

# Referências 
